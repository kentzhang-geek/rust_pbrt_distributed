// automatically generated by the FlatBuffers compiler, do not modify



use std::mem;
use std::cmp::Ordering;

extern crate flatbuffers;
use self::flatbuffers::{EndianScalar, Follow};

/// this flatbuffer scheme described all basic data structure that needs serialization
// struct Vec3d, aligned to 8
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct Vec3d(pub [u8; 24]);
impl Default for Vec3d { 
  fn default() -> Self { 
    Self([0; 24])
  }
}
impl std::fmt::Debug for Vec3d {
  fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
    f.debug_struct("Vec3d")
      .field("x", &self.x())
      .field("y", &self.y())
      .field("z", &self.z())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for Vec3d {}
impl flatbuffers::SafeSliceAccess for Vec3d {}
impl<'a> flatbuffers::Follow<'a> for Vec3d {
  type Inner = &'a Vec3d;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a Vec3d>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a Vec3d {
  type Inner = &'a Vec3d;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<Vec3d>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for Vec3d {
    type Output = Vec3d;
    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(self as *const Vec3d as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}
impl<'b> flatbuffers::Push for &'b Vec3d {
    type Output = Vec3d;

    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(*self as *const Vec3d as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for Vec3d {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}
impl<'a> Vec3d {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    x: f64,
    y: f64,
    z: f64,
  ) -> Self {
    let mut s = Self([0; 24]);
    s.set_x(x);
    s.set_y(y);
    s.set_z(z);
    s
  }

  pub fn x(&self) -> f64 {
    let mut mem = core::mem::MaybeUninit::<f64>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[0..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<f64>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_x(&mut self, x: f64) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const f64 as *const u8,
        self.0[0..].as_mut_ptr(),
        core::mem::size_of::<f64>(),
      );
    }
  }

  pub fn y(&self) -> f64 {
    let mut mem = core::mem::MaybeUninit::<f64>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[8..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<f64>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_y(&mut self, x: f64) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const f64 as *const u8,
        self.0[8..].as_mut_ptr(),
        core::mem::size_of::<f64>(),
      );
    }
  }

  pub fn z(&self) -> f64 {
    let mut mem = core::mem::MaybeUninit::<f64>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[16..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<f64>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_z(&mut self, x: f64) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const f64 as *const u8,
        self.0[16..].as_mut_ptr(),
        core::mem::size_of::<f64>(),
      );
    }
  }

}

// struct BoundingBox, aligned to 8
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct BoundingBox(pub [u8; 48]);
impl Default for BoundingBox { 
  fn default() -> Self { 
    Self([0; 48])
  }
}
impl std::fmt::Debug for BoundingBox {
  fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
    f.debug_struct("BoundingBox")
      .field("p_min", &self.p_min())
      .field("p_max", &self.p_max())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for BoundingBox {}
impl flatbuffers::SafeSliceAccess for BoundingBox {}
impl<'a> flatbuffers::Follow<'a> for BoundingBox {
  type Inner = &'a BoundingBox;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a BoundingBox>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a BoundingBox {
  type Inner = &'a BoundingBox;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<BoundingBox>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for BoundingBox {
    type Output = BoundingBox;
    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(self as *const BoundingBox as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}
impl<'b> flatbuffers::Push for &'b BoundingBox {
    type Output = BoundingBox;

    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(*self as *const BoundingBox as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for BoundingBox {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}
impl<'a> BoundingBox {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    p_min: &Vec3d,
    p_max: &Vec3d,
  ) -> Self {
    let mut s = Self([0; 48]);
    s.set_p_min(&p_min);
    s.set_p_max(&p_max);
    s
  }

  pub fn p_min(&self) -> &Vec3d {
    unsafe { &*(self.0[0..].as_ptr() as *const Vec3d) }
  }

  pub fn set_p_min(&mut self, x: &Vec3d) {
    self.0[0..0+24].copy_from_slice(&x.0)
  }

  pub fn p_max(&self) -> &Vec3d {
    unsafe { &*(self.0[24..].as_ptr() as *const Vec3d) }
  }

  pub fn set_p_max(&mut self, x: &Vec3d) {
    self.0[24..24+24].copy_from_slice(&x.0)
  }

}

// struct Vec4d, aligned to 8
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct Vec4d(pub [u8; 32]);
impl Default for Vec4d { 
  fn default() -> Self { 
    Self([0; 32])
  }
}
impl std::fmt::Debug for Vec4d {
  fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
    f.debug_struct("Vec4d")
      .field("x", &self.x())
      .field("y", &self.y())
      .field("z", &self.z())
      .field("w", &self.w())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for Vec4d {}
impl flatbuffers::SafeSliceAccess for Vec4d {}
impl<'a> flatbuffers::Follow<'a> for Vec4d {
  type Inner = &'a Vec4d;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a Vec4d>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a Vec4d {
  type Inner = &'a Vec4d;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<Vec4d>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for Vec4d {
    type Output = Vec4d;
    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(self as *const Vec4d as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}
impl<'b> flatbuffers::Push for &'b Vec4d {
    type Output = Vec4d;

    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(*self as *const Vec4d as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for Vec4d {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}
impl<'a> Vec4d {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    x: f64,
    y: f64,
    z: f64,
    w: f64,
  ) -> Self {
    let mut s = Self([0; 32]);
    s.set_x(x);
    s.set_y(y);
    s.set_z(z);
    s.set_w(w);
    s
  }

  pub fn x(&self) -> f64 {
    let mut mem = core::mem::MaybeUninit::<f64>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[0..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<f64>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_x(&mut self, x: f64) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const f64 as *const u8,
        self.0[0..].as_mut_ptr(),
        core::mem::size_of::<f64>(),
      );
    }
  }

  pub fn y(&self) -> f64 {
    let mut mem = core::mem::MaybeUninit::<f64>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[8..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<f64>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_y(&mut self, x: f64) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const f64 as *const u8,
        self.0[8..].as_mut_ptr(),
        core::mem::size_of::<f64>(),
      );
    }
  }

  pub fn z(&self) -> f64 {
    let mut mem = core::mem::MaybeUninit::<f64>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[16..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<f64>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_z(&mut self, x: f64) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const f64 as *const u8,
        self.0[16..].as_mut_ptr(),
        core::mem::size_of::<f64>(),
      );
    }
  }

  pub fn w(&self) -> f64 {
    let mut mem = core::mem::MaybeUninit::<f64>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[24..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<f64>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_w(&mut self, x: f64) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const f64 as *const u8,
        self.0[24..].as_mut_ptr(),
        core::mem::size_of::<f64>(),
      );
    }
  }

}

// struct Matrix44d, aligned to 8
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct Matrix44d(pub [u8; 128]);
impl Default for Matrix44d { 
  fn default() -> Self { 
    Self([0; 128])
  }
}
impl std::fmt::Debug for Matrix44d {
  fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
    f.debug_struct("Matrix44d")
      .field("idx", &self.idx())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for Matrix44d {}
impl flatbuffers::SafeSliceAccess for Matrix44d {}
impl<'a> flatbuffers::Follow<'a> for Matrix44d {
  type Inner = &'a Matrix44d;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a Matrix44d>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a Matrix44d {
  type Inner = &'a Matrix44d;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<Matrix44d>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for Matrix44d {
    type Output = Matrix44d;
    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(self as *const Matrix44d as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}
impl<'b> flatbuffers::Push for &'b Matrix44d {
    type Output = Matrix44d;

    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(*self as *const Matrix44d as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for Matrix44d {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}
impl<'a> Matrix44d {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    idx: &[Vec4d; 4],
  ) -> Self {
    let mut s = Self([0; 128]);
    s.set_idx(&idx);
    s
  }

  pub fn idx(&'a self) -> flatbuffers::Array<'a, Vec4d, 4> {
    flatbuffers::Array::follow(&self.0, 0)
  }

  pub fn set_idx(&mut self, x: &[Vec4d; 4]) {
    unsafe {
      std::ptr::copy(
        x.as_ptr() as *const u8,
        self.0.as_mut_ptr().add(0),
        128,
      );
    }
  }

}

